{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建神经网络\n",
    "神经网络由对数据执行操作的层/模块组成。 torch.nn 命名空间提供了构建您自己的神经网络所需的所有构建块。PyTorch 中的每个模块都是 nn.Module 的子类。神经网络本身就是一个模块，它由其他模块（层）组成。这种嵌套结构允许轻松构建和管理复杂的架构。\n",
    "\n",
    "在以下部分中，我们将构建一个神经网络来对 FashionMNIST 数据集中的图像进行分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取训练设备\n",
    "我们希望能够在硬件加速器（如 GPU 或 MPS）上训练我们的模型，如果可用的话。让我们检查一下 torch.cuda 或 torch.backends.mps 是否可用，否则我们将使用 CPU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cpu\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"device {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型类\n",
    "我们通过继承`nn.Module`来定义我们的神经网络，并在 `__init__` 中初始化神经网络层。每个 `nn.Module` 子类都在 `forward` 方法中实现对输入数据的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layout = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        out = self.layout(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们创建一个NeuralNetwork实例并将其移动到device上："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (layout): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了使用模型，我们将输入数据传递给它。这将执行模型的 forward 方法，以及一些 后台操作。不要直接调用 model.forward()！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在输入上调用模型会返回一个二维张量，其中 dim=0 对应于每个类的 10 个原始预测值的每个输出，dim=1 对应于每个输出的各个值。我们通过将其传递给 nn.Softmax 模块的实例来获得预测概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0806, -0.0381,  0.0002,  0.1662, -0.0061, -0.0794,  0.0175, -0.1399,\n",
      "          0.0622,  0.0619]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.1067, 0.0947, 0.0984, 0.1162, 0.0978, 0.0909, 0.1002, 0.0856, 0.1047,\n",
      "         0.1047]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([3])\n",
      "Predicted class: tensor([3])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "# print(X)\n",
    "\n",
    "out = model(X)\n",
    "print(out)\n",
    "\n",
    "pred_prob = nn.Softmax(dim=1)(out)\n",
    "print(pred_prob)\n",
    "\n",
    "y_pred = pred_prob.argmax(1)\n",
    "print(y_pred)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型层\n",
    "让我们分解 FashionMNIST 模型中的层。为了说明这一点，我们将获取一个大小为 28x28 的 3 张图像的样本 minibatch，并查看当我们将其传递给网络时会发生什么。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3, 28, 28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Flatten\n",
    "我们初始化 nn.Flatten 层，将每个 2D 28x28 图像转换为 784 个像素值的连续数组（minibatch 维度（在 dim=0 处）保持不变）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 784])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "flat_image.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Linear\n",
    "线性层 是一个模块，它使用其存储的权重和偏差对输入应用线性变换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=784, out_features=20, bias=True)\n",
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "print(layer1)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.ReLU\n",
    "非线性激活函数是在模型的输入和输出之间创建复杂映射的原因。它们应用于线性变换之后以引入_非线性_，帮助神经网络学习各种现象。\n",
    "\n",
    "在这个模型中，我们在线性层之间使用 nn.ReLU，但还有其他激活函数可以在你的模型中引入非线性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[-0.2990, -0.7515,  0.2314,  0.2621, -0.0281,  0.3658, -0.7137,  0.2839,\n",
      "         -0.6108,  0.1043,  0.1417,  0.2000,  0.1022, -0.3582,  0.3667, -0.0601,\n",
      "         -0.6928, -0.4936,  0.1503, -0.0134],\n",
      "        [ 0.0666, -0.5275,  0.1474,  0.3404,  0.0647,  0.4545, -0.8173,  0.1863,\n",
      "         -0.4470,  0.0584, -0.0381,  0.6315,  0.0937, -0.3427,  0.4257, -0.1828,\n",
      "         -0.6695, -0.4099, -0.0665, -0.1068],\n",
      "        [-0.0972, -0.6145, -0.0150,  0.0800, -0.0684,  0.6106, -0.9313,  0.0690,\n",
      "         -0.7143,  0.0346,  0.1515,  0.4812,  0.3094, -0.1760,  0.2195, -0.1643,\n",
      "         -0.8402, -0.3973,  0.0037,  0.2351]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0000, 0.0000, 0.2314, 0.2621, 0.0000, 0.3658, 0.0000, 0.2839, 0.0000,\n",
      "         0.1043, 0.1417, 0.2000, 0.1022, 0.0000, 0.3667, 0.0000, 0.0000, 0.0000,\n",
      "         0.1503, 0.0000],\n",
      "        [0.0666, 0.0000, 0.1474, 0.3404, 0.0647, 0.4545, 0.0000, 0.1863, 0.0000,\n",
      "         0.0584, 0.0000, 0.6315, 0.0937, 0.0000, 0.4257, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0800, 0.0000, 0.6106, 0.0000, 0.0690, 0.0000,\n",
      "         0.0346, 0.1515, 0.4812, 0.3094, 0.0000, 0.2195, 0.0000, 0.0000, 0.0000,\n",
      "         0.0037, 0.2351]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Sequential\n",
    "nn.Sequential 是一个有序的模块容器。数据按定义的相同顺序传递给所有模块。你可以使用顺序容器来组合一个像 seq_modules 这样的快速网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10])\n"
     ]
    }
   ],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20,10)\n",
    ")\n",
    "\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)\n",
    "print(logits.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.softmax\n",
    "神经网络的最后一个线性层返回_logits_ - [-infty, infty] 中的原始值 - 这些值被传递给 nn.Softmax 模块。logits 被缩放到 [0, 1] 之间的值，表示模型对每个类的预测概率。 dim 参数指示值必须沿其求和为 1 的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0775, 0.1110, 0.1050, 0.0935, 0.1191, 0.0733, 0.0791, 0.0945, 0.1328,\n",
      "         0.1143],\n",
      "        [0.0739, 0.0959, 0.1058, 0.0931, 0.1407, 0.0627, 0.0717, 0.1000, 0.1236,\n",
      "         0.1326],\n",
      "        [0.0759, 0.1104, 0.0995, 0.0967, 0.1198, 0.0840, 0.0745, 0.1001, 0.1164,\n",
      "         0.1226]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "res = nn.Softmax(dim=1)(logits)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型参数\n",
    "神经网络中的许多层都是_参数化的_，即具有在训练期间优化的相关权重和偏差。继承 nn.Module 会自动跟踪模型对象中定义的所有字段，并可以使用模型的 parameters() 或 named_parameters() 方法访问所有参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (layout): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: layout.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 2.0048e-03,  3.0478e-02,  2.0277e-02,  ...,  3.2663e-03,\n",
      "         -2.7711e-02,  1.4063e-02],\n",
      "        [ 3.9998e-05, -1.3247e-02, -1.8281e-02,  ...,  6.2658e-03,\n",
      "          2.2275e-02,  2.2839e-02]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: layout.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0256,  0.0276], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: layout.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0429, -0.0170, -0.0161,  ...,  0.0203, -0.0123, -0.0330],\n",
      "        [ 0.0011,  0.0134,  0.0193,  ...,  0.0180, -0.0062,  0.0281]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: layout.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0130, -0.0233], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: layout.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0419, -0.0350,  0.0103,  ..., -0.0238,  0.0063, -0.0288],\n",
      "        [-0.0208, -0.0232, -0.0107,  ..., -0.0203,  0.0326,  0.0013]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: layout.4.bias | Size: torch.Size([10]) | Values : tensor([0.0371, 0.0432], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
